{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ = pd.read_feather('../input/amexfeather/train_data.ftr')\n",
    "# Keep the latest statement features for each customer\n",
    "train_dataset = train_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()\n",
    "# train_dataset = train_dataset.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset_\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = train_dataset.isnull().sum() * 100 / len(train_dataset)\n",
    "missing_value_df = pd.DataFrame({'column_name': train_dataset.columns,\n",
    "                                 'percent_missing': percent_missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_null_columns = missing_value_df[missing_value_df['percent_missing'] > 0].column_name.to_list()\n",
    "len(high_null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_df[missing_value_df['percent_missing'] < 75].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_df[missing_value_df['percent_missing'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns with high null value percentages\n",
    "train_dataset = train_dataset.drop(['S_2','D_66','D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_ = pd.read_feather('../input/amexfeather/test_data.ftr')\n",
    "# Keep the latest statement features for each customer\n",
    "test_dataset = test_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()\n",
    "# test_dataset = test_dataset.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_dataset_\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.drop(['S_2','D_42','D_49','D_66','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = train_dataset.select_dtypes(exclude = 'number').columns.to_list()\n",
    "print(categorical_columns)\n",
    "print(len(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical columns\n",
    "num_cols = [col for col in train_dataset.columns if col not in categorical_columns + [\"target\"]]\n",
    "\n",
    "print(f'Total number of features: {(train_dataset.shape[1])-1}')\n",
    "print(f'Total number of categorical features: {len(categorical_cols)}')\n",
    "print(f'Total number of numerical features: {len(num_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = train_dataset.columns[train_dataset.isna().any()].tolist()\n",
    "num_null_columns = set(null_cols) - set(categorical_cols)\n",
    "cat_null_cols = set(null_cols) - set(num_cols)\n",
    "print(len(null_cols)),print(len(num_null_columns)), print(len(cat_null_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(num_null_columns):\n",
    "    train_dataset[col] = train_dataset[col].fillna(train_dataset[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col2 in list(cat_null_cols):\n",
    "    train_dataset[col2] = train_dataset[col2].astype('category').cat.add_categories('unknown')\n",
    "    train_dataset[col2] = train_dataset[col2].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.columns.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = test_dataset.columns[test_dataset.isna().any()].tolist()\n",
    "num_null_columns = set(null_cols) - set(categorical_cols)\n",
    "cat_null_cols = set(null_cols) - set(num_cols)\n",
    "print(len(null_cols)),print(len(num_null_columns)), print(len(cat_null_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in list(num_null_columns):\n",
    "    test_dataset[column] = test_dataset[column].fillna(train_dataset[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column2 in list(cat_null_cols):\n",
    "    test_dataset[column2] = test_dataset[column2].astype('category').cat.add_categories('unknown')\n",
    "    test_dataset[column2] =  test_dataset[column2].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.columns.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset.shape)\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_without_target = train_dataset.drop([\"target\"],axis=1)\n",
    "\n",
    "# cor_matrix = train_dataset_without_target.corr()\n",
    "# col_core = set()\n",
    "\n",
    "# for i in range(len(cor_matrix.columns)):\n",
    "#     for j in range(i):\n",
    "#         if(cor_matrix.iloc[i, j] > 0.9):\n",
    "#             col_name = cor_matrix.columns[i]\n",
    "#             col_core.add(col_name)\n",
    "# col_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.drop(col_core, axis=1)\n",
    "# test_dataset = test_dataset.drop(col_core, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_dataset.shape)\n",
    "# print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_dataset['target']\n",
    "train_dataset = train_dataset.drop(['target'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X shape is = {train_dataset.shape}\" )\n",
    "print(f\"Y shape is = {y.shape}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding ctaegorical columns and reindex test data set's columns with train datas columns\n",
    "train_dataset = pd.get_dummies(train_dataset, columns = categorical_columns)\n",
    "test_dataset = pd.get_dummies(test_dataset, columns = categorical_columns)\n",
    "\n",
    "test_dataset = test_dataset.reindex(columns = train_dataset.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in categorical_cols:\n",
    "#     test_dataset[col] =test_dataset[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# selector_model = xgb.XGBClassifier()\n",
    "# selector_model.fit(train_dataset, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 150\n",
    "# top_k_features = selector_model.feature_importances_.argsort()[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = selector_model.feature_importances_\n",
    "feat_imp_df = pd.read_csv('/kaggle/input/feature-importances/feature_importances.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueeee = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv')\n",
    "trueeee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_imp_df = pd.DataFrame(columns = ['feature','imp'])\n",
    "# feat_imp_df['feature'] = (train_dataset.columns)\n",
    "# feat_imp_df['imp'] = feature_importances\n",
    "# feat_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_imp_df.to_csv('feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for col,score in zip(train_dataset.columns,selector_model.feature_importances_):\n",
    "# #     print(col,score)\n",
    "\n",
    "# col_id_map = {}\n",
    "# for col_id, col_name in enumerate(train_dataset.columns):\n",
    "#     col_id_map[col_id] = col_name\n",
    "# print(col_id_map)\n",
    "\n",
    "# top_features_names= [col_id_map[col_id] for col_id in top_k_features[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_imp_df = feat_imp_df.sort_values(by=['imp'], ascending=False)\n",
    "# feat_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_names = feat_imp_df['feature'].values[:number_of_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['R_8', 'R_16', 'R_15', 'B_33', 'R_4', 'S_22', 'D_78', 'D_117_-1.0', 'D_125', 'S_13', 'D_104', 'D_83', 'R_10', 'R_24', 'D_86', 'D_107', 'R_20', 'D_127', 'D_80', 'D_139', 'R_14', 'S_27', 'D_141', 'D_69', 'D_113', 'D_68_3.0', 'D_96', 'R_17', 'B_15', 'B_38_7.0', 'D_102', 'D_105', 'B_30_2.0', 'B_41', 'D_91', 'S_6', 'R_6', 'D_129', 'D_75', 'D_145', 'D_58', 'D_72', 'D_115', 'D_144', 'D_61', 'D_70', 'D_65', 'S_12', 'D_74', 'D_55', 'D_128', 'B_28', 'D_117_3.0', 'D_103', 'D_124', 'B_40', 'B_12', 'D_140', 'S_25', 'B_8', 'B_25', 'B_21', 'D_117_6.0', 'R_11', 'B_13', 'D_68_2.0', 'B_24', 'S_16', 'D_118', 'D_119', 'R_12', 'B_6', 'B_19', 'D_117_2.0', 'D_52', 'D_143', 'S_15', 'D_122', 'D_71', 'P_4', 'D_62', 'B_16', 'S_11', 'D_60', 'B_36', 'S_5', 'S_8', 'R_7', 'D_53', 'B_14', 'D_121', 'S_9', 'B_17', 'D_117_1.0', 'B_20', 'D_59', 'D_117_5.0', 'S_7', 'B_26', 'D_82']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset[top_features_names]\n",
    "# test_dataset = test_dataset[top_features_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset.shape)\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( train_dataset, y, test_size=0.3, random_state=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_v1 = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(test_dataset_v1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = (y_train.value_counts()[0])/(y_train.value_counts()[1])\n",
    "scale_pos_weight_full = (y.value_counts()[0])/(y.value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# oversample = SMOTE()\n",
    "# X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling the minority class to solve class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "train_dataset, y = oversample.fit_resample(train_dataset, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM classifier - Linear\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1)).fit(train_dataset)\n",
    "train_dataset = scaler.transform(train_dataset)\n",
    "test_dataset_v1 = scaler.transform(test_dataset_v1)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "\n",
    "clf.fit(train_dataset, y.ravel())\n",
    "\n",
    "predictions_svc = clf._predict_proba_lr(test_dataset_v1)\n",
    "\n",
    "predictions_svc = predictions_svc[:,1]\n",
    "sample_dataset = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n",
    "output = pd.DataFrame({'customer_ID': sample_dataset.customer_ID, 'prediction': predictions_svc})\n",
    "output.to_csv('Submission SVM full.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection to reduce dimensionality for knn\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "selector = SelectKBest(mutual_info_classif, k=20)\n",
    "selector.fit(train_dataset, y)\n",
    "mask = selector.get_support()\n",
    "new_features = train_dataset.columns[mask]\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = selector.transform(train_dataset)\n",
    "test_dataset_v1 = selector.transform(test_dataset_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape, test_dataset_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Knn classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn_clf.fit(train_dataset, y)\n",
    "predictions_knn = knn_clf.predict_proba(test_dataset_v1)\n",
    "\n",
    "predictions_knn = predictions_knn[:,1]\n",
    "sample_dataset = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n",
    "output = pd.DataFrame({'customer_ID': sample_dataset.customer_ID, 'prediction': predictions_knn})\n",
    "output.to_csv('Submission KNN full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(n_estimators = 500, \n",
    "                                objective = 'reg:logistic', \n",
    "                                seed = 69, \n",
    "#                                 scale_pos_weight = scale_pos_weight,\n",
    "                                colsample_bytree=0.8,\n",
    "#                                 min_child_weight=3,\n",
    "                                max_depth = 5,\n",
    "                                subsample = 0.8,\n",
    "                               learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(train_dataset, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = final_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_v1 = p[:,1].reshape(-1,1)\n",
    "\n",
    "y_pred = pd.DataFrame(p_v1, columns = ['prediction'])\n",
    "# y_pred.head()\n",
    "\n",
    "y_true = y_test.to_frame(name = 'target')\n",
    "y_true = y_true.reset_index(drop=True)\n",
    "# y_true.head()\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "roc_auc_score(y_test, p_v1)\n",
    "\n",
    "amex_metric(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7634980539964447\n",
    "0.7637116591259403\n",
    "0.7712345331426667 - best 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9485239523284494\n",
    "0.956661835314092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting roc curve\n",
    "def plot_roc(y_true, y_score, label_name, ax):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax.set_ylabel('TPR')\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_title(\n",
    "        f\"{label_name}: AUC = {roc_auc_score(y_true, y_score):.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "plot_roc(\n",
    "    y_test, \n",
    "    p_v1, \n",
    "    'y',\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = final_model.predict_proba(test_dataset_v1)\n",
    "predictions\n",
    "\n",
    "predictions = predictions[:,1]\n",
    "\n",
    "sample_dataset = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n",
    "output = pd.DataFrame({'customer_ID': sample_dataset.customer_ID, 'prediction': predictions})\n",
    "output.to_csv('submission_v5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
